# Hierarchical-Clustering
Hierarchical Clustering

מה עושים כאן?

מייבאים חבילות לעבודה:

numpy, pandas — חישובים וטעינת נתונים.

matplotlib.pyplot — גרפים (דנדרוגרמה, וכו’).

StandardScaler — סטנדרטיזציה (ממוצע 0, סטיית תקן 1) לכל פיצ’ר.

AgglomerativeClustering, linkage, dendrogram, fcluster — כלים ל־Hierarchical Clustering; linkage/dendrogram/fcluster הם הגרסה ה”קלאסית” מ־SciPy, ו־AgglomerativeClustering היא גרסת sklearn.

KMeans — קלאסטרינג נוסף לצורך השוואה.

PCA — הורדת מימדיות ובדיקת וריאנס מוסבר.

הפרמטרים של matplotlib (plt.rcParams) נותנים גודל תרשים קבוע וgrid כברירת מחדל, כדי שיהיה נוח לראות.

למה?

כלים אלו מכסים את כל השלבים: הכנת הדאטה ➜ היררכי ➜ KMeans ➜ PCA ➜ ויזואליזציה.

סטנדרטיזציה נדרשת משום שהמרחקים (אוקלידי/Ward) ו־PCA רגישים לסולמות שונים בין פיצ’רים.

1) Load the CSV into a DataFrame

מה עושים כאן?

בודקים שני מסלולים אפשריים לקובץ: CSV/wine_dataset.csv (כפי שנכתב בתרגיל) או /mnt/data/wine_dataset.csv (העלאה לשיחה).

אם הקובץ לא נמצא — זורקים FileNotFoundError עם הודעה ברורה.

pd.read_csv(csv_path) טוען את הנתונים ל־df, מדפיס shape ומציג head() (ב־Notebook יוצג, בקובץ .py כדאי לעשות print(df.head()) אם רוצים פלט).

למה?

לעיתים סביבת הריצה שונה (מחשב מקומי/ענן/שיתוף). בדיקת כמה מסלולים חוסכת שבירת קוד.

הדפסת shape/head מוודאת שטענו נתונים בצורה תקינה ושיש את העמודות שציפינו.

2) Keep numeric columns only

מה עושים כאן?

df.select_dtypes(include=[np.number]) שומר רק עמודות מספריות — כיוון שקלאסטרינג ו־PCA מצפים למטריצה נומרית.

dropna() — מסירים שורות עם חסרים (פתרון פשוט לתרגיל; בעולם אמיתי נעדיף אימפיוטציה).

שומרים את האינדקסים של השורות שנשמרו (idx_kept) כדי שנוכל להחזיר את תוויות הקלאסטר ל־df במקומות הנכונים.

למה?

אלגוריתמי מרחק ו־PCA לא עובדים על טקסט ישירות. אם יש קטגוריות, עושים להן One-Hot לפני.

שמירת idx_kept מונעת ערבוב בין שמות/שורות המקור לבין מטריצת העבודה (שייתכן שנוקתה).

3) Scale numeric features

מה עושים כאן?

מפעילים StandardScaler על המטריצה המספרית: fit_transform מחזיר X_scaled.

מדפיסים את ה־shape כדי לוודא תאימות.

למה?

קריטי בקלאסטרינג מבוסס מרחק וב־PCA: בלי סקיילינג, פיצ’ר עם טווח ערכים גדול “ישלוט” על המרחק/קווריאנס.

StandardScaler מתאים בדרך כלל ל־PCA (מרכוז סביב 0 הופך את פירוש הקומפוננטים לנקי יותר).

הערה: אפשר להחליף ל־MinMaxScaler אם יש צורך ב־[0,1], אבל ל־PCA/מרחקים סטנדרטיזציה היא ברירת מחדל טובה.

4) Hierarchical Clustering (Ward linkage)

מה עושים כאן?

מחשבים מטריצת קשרים Z = linkage(X_scaled, method="ward").

מציירים דנדרוגרמה עם dendrogram(Z) — עץ איחודים מה־Bottom-Up.

למה Ward?

ward מאחד קלאסטרים כך שהעלייה ב־Within-Cluster SSE תהיה מינימלית בכל צעד. זה מוצא קלאסטרים קומפקטיים ודומה ברוחו לקיי-מינז (מבחינת פונקציית מטרה).

הדנדרוגרמה מאפשרת לבחור גובה חיתוך (כלומר, כמה קבוצות נרצה) לפי “קפיצה” גדולה במרחק האיחוד.

מה רואים בדנדרוגרמה?

הציר האנכי = מרחק/עלייה ב־SSE בעת האיחוד.

חתך אופקי (קו) יגדיר מספר קבוצות: כמה ענפים נחצים.

5) Choose K and cut the tree

מה עושים כאן?

בוחרים k — ערך התחלתי (למשל 3), אבל חשוב להתאים אותו לדנדרוגרמה בפועל.

fcluster(Z, t=k, criterion="maxclust") “חותך” את העץ ל־k קלאסטרים ומחזיר תוויות 1..k.

מחזירים את התוויות ל־df במקומות של idx_kept בעמודה cluster_hier.

למה?

fcluster נותן לנו תיוג שטוח (Flat Clustering) לאחר שבנינו עץ היררכי.

השמה ל־df שומרת את התוויות ליד הנתונים המקוריים, כדי שנוכל לסכום/לפרש.

6) Counts per cluster (Hierarchical)

מה עושים כאן?

value_counts().sort_index() על cluster_hier כדי לספור כמה דגימות יש בכל קלאסטר.

למה?

שלב חשוב להערכת ה־איזון בין הקבוצות, זיהוי קלאסטרים קטנים מאוד (אולי Outliers/נישה) לעומת קלאסטרים גדולים.

7) (Bonus A) K-Means with the same K

מה עושים כאן?

מריצים KMeans על אותם נתונים מנורמלים (X_scaled) עם אותו k.

n_init=10 — מריץ את האלגוריתם מכמה אתחולים (centroids) ומקבל תוצאה יציבה יותר.

random_state=42 — שחזוריות.

התוויות של KMeans הן 0..k-1, אז מוסיפים +1 לנוחות השוואה.

מדפיסים value_counts ומשווים לטבלת ההיררכי.

למה הספירות לא חייבות להיות זהות?

אלגוריתמים שונים ומטרות שונות:

Ward = איחוד גרידי להפחתת עלייה ב־SSE;

KMeans = אופטימיזציה איטרטיבית סביב סנטראוידים, תלויה באתחול.

Labels הם שרירותיים: “קלאסטר 1” בהיררכי לא חייב להיות “קלאסטר 1” ב־KMeans.

כדי להשוות “באמת” התאמה בין תוויות, משתמשים לעיתים ברה-מספור (Hungarian algorithm) או במדדים כמו Adjusted Rand Index.

8) (Bonus B) PCA for Dimensionality Reduction

מה עושים כאן?

מריצים PCA() על X_scaled, מקבלים X_pca, ואת explained_variance_ratio_.

מחשבים אחוז מוסבר לכל רכיב (%) ואת המצטבר (cumsum).

מחפשים כמה רכיבים נדרשים כדי להגיע ל־~90% ו־~95% וריאנס (משתנים n90, n95).

למה?

PCA מייצרת צירים אורתוגונליים שמסבירים הכי הרבה וריאנס ראשונים.

מאפשר לצמצם ממדיות (Noise reduction / מהירות) ולשפר לפעמים קלאסטרינג.

חשוב: ביצענו קודם StandardScaler כדי שהקווריאנס לא יישלט ע"י פיצ’רים רחבים.

9) (Optional) Cluster on PCA-Reduced Space

מה עושים כאן?

בוחרים מספר רכיבים (למשל n95) וחותכים את X_pca לעמודות הראשונות (X_red).

מריצים שוב היררכי (Ward) ו־KMeans על X_red.

מדפיסים תוויות ייחודיות לבדיקה.

למה?

עבודה על מרחב מצומצם יכולה:

להפחית רעש ולחדד מבנה,

להאיץ חישוב,

לעזור כש־p גדול.

עם זאת, לפעמים דליפה של מידע/צמצום יתר ישנו את הפילוח — לכן כדאי לבדוק יציבות ולשקול בחירה מחדש של k (למשל דנדרוגרמה/סילואט) במרחב המצומצם.

10) Quick Notes

עיקרי הדברים שמחברים הכל יחד:

בחירת K בהיררכי: לפי הדנדרוגרמה — מחפשים “קפיצה” משמעותית בגובה האיחוד ומציבים שם את החיתוך.

הבדלי ספירות/תיוגים בין Hierarchical ל־KMeans — נובעים ממטרות שונות, אתחול, ושמות קלאסטרים שרירותיים. זה צפוי.

סקיילינג תמידי לפני PCA/מרחקים.

PCA — בוחר כמה קומפוננטים לפי % וריאנס מוסבר (למשל 90–95%).

לשימוש תעשייתי: שקול Pipeline (Scaler → PCA → Clustering), מדדי איכות (Silhouette/CH/DB), וטיפול חסרים/Outliers בצורה עקבית.

סיכום קצר (TL;DR)

ניקינו לעמודות נומריות, סטנדרטיזציה → לאזן סולמות.

Hierarchical (Ward): בנינו linkage וציירנו דנדרוגרמה כדי לבחור k; חתכנו עם fcluster.

ספרנו דגימות בכל קלאסטר לבחינת איזון.

KMeans עם אותו k להשוואה — טבעי שלא ייצא זהה (אלגוריתמים שונים/תיוג שרירותי).

PCA: בדקנו וריאנס מוסבר ומצטבר, קבענו כמה PCs לשמירה, והדגמנו גם קלאסטרינג במרחב ה־PCs
